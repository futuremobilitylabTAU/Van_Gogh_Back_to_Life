{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fda15a9-98bc-42a9-ad90-4fd5aa4918b4",
   "metadata": {},
   "source": [
    "Import : First the standard library modules (e.g., os, gc, json), then data analysis libraries (numpy, pandas, matplotlib, PIL), followed by PyTorch/Torchvision imports (torch, nn, optim, torchvision, etc.), and finally additional packages like optuna, wandb, and sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "969ea41c-b940-45ab-8041-17a67d22ea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/gabriel/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabi-dadashev\u001b[0m (\u001b[33mgabi-dadashev-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import unidecode\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import VGG19_Weights\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "######## \n",
    "\n",
    "from DATA_AND_MODELS import VanGoghDataset,VanGoghModel\n",
    "\n",
    "wandb.login(key='ba8ed449ca151ad3f490026aec87d75b7171a16d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b53c0-cf83-4d2c-9b19-98345782d8bd",
   "metadata": {},
   "source": [
    "Load the CSV, keep selected columns, filter only **Post-Impressionism** images, add a 'is_van_gogh' flag, and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4371f5c-7bf4-4922-875a-d329a4fe725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_csv(\"classes.csv\", encoding=\"utf-8\")\n",
    "classes=classes[['filename', 'artist', 'genre', 'description', 'phash', 'width','height', 'genre_count']].copy()\n",
    "classes_only_post_impressionism = classes[classes[\"filename\"].str.contains('Post_Impressionism', case=False, na=False)].copy()\n",
    "classes_only_post_impressionism['is_van_gogh'] = np.where(classes_only_post_impressionism['artist'] == 'vincent van gogh', 1, 0)\n",
    "classes_only_post_impressionism=classes_only_post_impressionism.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85333b40-a697-4367-9b5e-0ca778a23bc4",
   "metadata": {},
   "source": [
    "Free unused GPU cache with PyTorch and invoke Python's garbage collector to release unreferenced objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6de9961-0d23-4245-bf97-9ed144253176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f17b52de-5374-4429-8077-1183ceb3fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VanGoghDataset(dataframe=classes_only_post_impressionism)\n",
    "model_VGG19 = VanGoghModel(device,None,'VGG19')\n",
    "model_Alex = VanGoghModel(device,None,'AlexNet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1321e726-9997-4f04-98a1-5c8d89536158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop_check(patience, best_val_loss, best_val_loss_epoch, current_val_loss, current_val_loss_epoch):\n",
    "    early_stop_flag = False  \n",
    "    if current_val_loss < best_val_loss:\n",
    "        best_val_loss = current_val_loss\n",
    "        best_val_loss_epoch = current_val_loss_epoch\n",
    "    else:\n",
    "        if current_val_loss_epoch - best_val_loss_epoch > patience:\n",
    "            early_stop_flag = True  \n",
    "    return best_val_loss, best_val_loss_epoch, early_stop_flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51d6168b-a9e6-4424-a2f5-4fe8fbbf9b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs, patience, trial):\n",
    "\n",
    "    print('train_model_with_hyperparams')\n",
    "    \n",
    "    \n",
    "    best_val_loss = float('inf')  \n",
    "    best_val_loss_epoch = 0  \n",
    "    early_stop_flag = False\n",
    "    best_model_state = None  \n",
    "\n",
    "    ### Epoch Loop iterate the all data observation X time\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train() \n",
    "        train_loss = 0.0 \n",
    "        total_train_samples = 0 \n",
    "        correct_train_predictions = 0 \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "           \n",
    "            \n",
    "            outputs = model(inputs).squeeze(1)  \n",
    "            loss = criterion(outputs.view(-1), labels)\n",
    "\n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            total_train_samples += inputs.size(0)\n",
    "\n",
    "      \n",
    "            predicted = (outputs > 0).float() \n",
    "\n",
    "            correct_train_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= total_train_samples\n",
    "        train_accuracy = correct_train_predictions / total_train_samples\n",
    "\n",
    "        model.eval()  \n",
    "        val_loss = 0.0 \n",
    "        total_val_samples = 0 \n",
    "        correct_val_predictions = 0\n",
    "\n",
    "        ## Intenial-Batch Loop split the data to batches\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for inputs, labels in val_loader: \n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "               \n",
    "            \n",
    "               \n",
    "                loss = criterion(outputs.view(-1), labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                total_val_samples += inputs.size(0)\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        \n",
    "        ##Validation \n",
    "        \n",
    "        val_loss /= total_val_samples\n",
    "        val_accuracy = correct_val_predictions / total_val_samples\n",
    "\n",
    "        best_val_loss, best_val_loss_epoch, early_stop_flag = early_stop_check(patience, best_val_loss, best_val_loss_epoch, val_loss, epoch)\n",
    "\n",
    "        ##Early Stopping\n",
    "        \n",
    "        if val_loss == best_val_loss:\n",
    "            best_model_state = model.state_dict()\n",
    "        ### Weights & Biases (W&B) decimantion \n",
    "        \n",
    "        wandb.log({ \n",
    "            \"Epoch\": epoch,\n",
    "            \"Train Loss\": train_loss,\n",
    "            \"Train Accuracy\": train_accuracy,\n",
    "            \"Validation Loss\": val_loss,\n",
    "            \"Validation Accuracy\": val_accuracy\n",
    "        })\n",
    "\n",
    "        if early_stop_flag: \n",
    "            break \n",
    "\n",
    "        ### handle best model\n",
    "        \n",
    "\n",
    "    if best_model_state is not None: \n",
    "        torch.save(best_model_state, f\"best_model_trial_{trial.number}.pt\") \n",
    "   \n",
    "\n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a17f13ca-0dd4-45cc-a48a-52718d2cd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 4, 16, step=4)\n",
    "    patience = 7\n",
    "    k_folds = 5  \n",
    "    \n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_losses = []   \n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        \n",
    "        print(f\"--- Trial (Exp) {trial.number}, Fold {fold+1}/{k_folds} ---\")\n",
    "\n",
    "\n",
    "        train_dataset = Subset(dataset, train_idx)\n",
    "        val_dataset = Subset(dataset, val_idx)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model =  VanGoghModel(device,trial,None).model\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        wandb.init( project=\"VanGogh-Classifier\",\n",
    "        config={\n",
    "            \"model\": trial.params[\"model\"],  \n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"k_folds\": k_folds\n",
    "        },\n",
    "        name=f\"trial_{trial.number}_{trial.params['model']}\"\n",
    "    )\n",
    "\n",
    "        best_val_loss = train_model_with_hyperparams(\n",
    "            model, train_loader, val_loader, optimizer, criterion, epochs=4, patience=patience, trial=trial\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        fold_losses.append(best_val_loss)\n",
    "\n",
    "    avg_val_loss = sum(fold_losses) / len(fold_losses)\n",
    "    print(f\"Trial {trial.number}, Average Validation Loss across {k_folds} folds: {avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    ###Decumantation    \n",
    "\n",
    "    model_type = trial.params[\"model\"]\n",
    "    global BEST_PARAMS, BEST_PARAMS_HISTORY\n",
    "    if avg_val_loss < BEST_PARAMS[model_type][\"loss\"]:\n",
    "        BEST_PARAMS[model_type][\"loss\"] = avg_val_loss\n",
    "        BEST_PARAMS[model_type][\"params\"] = trial.params.copy()\n",
    "        \n",
    "        BEST_PARAMS_HISTORY[model_type].append({\n",
    "            \"trial\": trial.number,\n",
    "            \"loss\": avg_val_loss,\n",
    "            \"params\": trial.params.copy()\n",
    "        })\n",
    "        \n",
    "        with open(f\"best_params_history_{model_type}.json\", \"w\") as f:\n",
    "            json.dump(BEST_PARAMS_HISTORY[model_type], f, indent=4)\n",
    "        print(f\"New best parameters for {model_type} saved: {trial.params}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    del model \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21eb9936-7afd-4e98-a8f9-e326f6b84a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS = {\n",
    "    \"AlexNet\": {\"loss\": float(\"inf\"), \"params\": None},\n",
    "    \"VGG19\": {\"loss\": float(\"inf\"), \"params\": None}\n",
    "}\n",
    "\n",
    "\n",
    "BEST_PARAMS_HISTORY = {\n",
    "    \"AlexNet\": [],\n",
    "    \"VGG19\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8fc5376-2440-45b8-ab91-a4e95cb9c6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 11:47:06,289] A new study created in memory with name: no-name-11d97f53-6a9f-4595-8d33-6766a8c25252\n",
      "/tmp/ipykernel_8926/2823819912.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "/tmp/ipykernel_8926/2823819912.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Trial (Exp) 0, Fold 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gabriel/GIT_DL/wandb/run-20250218_114707-m8pf65aw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gabi-dadashev-tel-aviv-university/VanGogh-Classifier/runs/m8pf65aw' target=\"_blank\">trial_0_VGG19</a></strong> to <a href='https://wandb.ai/gabi-dadashev-tel-aviv-university/VanGogh-Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gabi-dadashev-tel-aviv-university/VanGogh-Classifier' target=\"_blank\">https://wandb.ai/gabi-dadashev-tel-aviv-university/VanGogh-Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gabi-dadashev-tel-aviv-university/VanGogh-Classifier/runs/m8pf65aw' target=\"_blank\">https://wandb.ai/gabi-dadashev-tel-aviv-university/VanGogh-Classifier/runs/m8pf65aw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 0, Fold 2/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 0, Fold 3/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 0, Fold 4/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 0, Fold 5/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:13:11,500] Trial 0 finished with value: 0.6931471116553085 and parameters: {'learning_rate': 5.313401180185412e-05, 'weight_decay': 1.116092803144322e-06, 'batch_size': 8, 'model': 'VGG19'}. Best is trial 0 with value: 0.6931471116553085.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Average Validation Loss across 5 folds: 0.6931\n",
      "New best parameters for VGG19 saved: {'learning_rate': 5.313401180185412e-05, 'weight_decay': 1.116092803144322e-06, 'batch_size': 8, 'model': 'VGG19'}\n",
      "--- Trial (Exp) 1, Fold 1/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 1, Fold 2/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 1, Fold 3/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 1, Fold 4/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 1, Fold 5/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:47:46,969] Trial 1 finished with value: 0.6779947851876976 and parameters: {'learning_rate': 2.2012211629918463e-05, 'weight_decay': 1.549668920821374e-06, 'batch_size': 4, 'model': 'VGG19'}. Best is trial 1 with value: 0.6779947851876976.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Average Validation Loss across 5 folds: 0.6780\n",
      "New best parameters for VGG19 saved: {'learning_rate': 2.2012211629918463e-05, 'weight_decay': 1.549668920821374e-06, 'batch_size': 4, 'model': 'VGG19'}\n",
      "--- Trial (Exp) 2, Fold 1/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 2, Fold 2/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 2, Fold 3/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 2, Fold 4/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 2, Fold 5/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 15:57:44,839] Trial 2 finished with value: 0.6739577318623196 and parameters: {'learning_rate': 3.020842532706549e-05, 'weight_decay': 1.2852718112074654e-05, 'batch_size': 8, 'model': 'AlexNet'}. Best is trial 2 with value: 0.6739577318623196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Average Validation Loss across 5 folds: 0.6740\n",
      "New best parameters for AlexNet saved: {'learning_rate': 3.020842532706549e-05, 'weight_decay': 1.2852718112074654e-05, 'batch_size': 8, 'model': 'AlexNet'}\n",
      "--- Trial (Exp) 3, Fold 1/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 3, Fold 2/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 3, Fold 3/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 3, Fold 4/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 3, Fold 5/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 17:18:57,053] Trial 3 finished with value: 0.6931471830977779 and parameters: {'learning_rate': 0.0004552863321751068, 'weight_decay': 1.9116044844065386e-05, 'batch_size': 16, 'model': 'VGG19'}. Best is trial 2 with value: 0.6739577318623196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Average Validation Loss across 5 folds: 0.6931\n",
      "--- Trial (Exp) 4, Fold 1/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 4, Fold 2/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 4, Fold 3/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 4, Fold 4/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 4, Fold 5/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 18:41:15,097] Trial 4 finished with value: 0.6803820243566787 and parameters: {'learning_rate': 1.3047395606841869e-05, 'weight_decay': 4.686447846662904e-06, 'batch_size': 16, 'model': 'VGG19'}. Best is trial 2 with value: 0.6739577318623196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Average Validation Loss across 5 folds: 0.6804\n",
      "--- Trial (Exp) 5, Fold 1/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 5, Fold 2/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 5, Fold 3/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 5, Fold 4/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 5, Fold 5/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 20:16:19,186] Trial 5 finished with value: 0.6931471617752092 and parameters: {'learning_rate': 0.00021480493851935012, 'weight_decay': 1.224368337346564e-05, 'batch_size': 4, 'model': 'VGG19'}. Best is trial 2 with value: 0.6739577318623196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Average Validation Loss across 5 folds: 0.6931\n",
      "--- Trial (Exp) 6, Fold 1/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 6, Fold 2/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 6, Fold 3/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 6, Fold 4/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 6, Fold 5/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 21:37:34,366] Trial 6 finished with value: 0.6931471830977779 and parameters: {'learning_rate': 0.0009653179054889212, 'weight_decay': 4.898571656609406e-06, 'batch_size': 16, 'model': 'VGG19'}. Best is trial 2 with value: 0.6739577318623196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Average Validation Loss across 5 folds: 0.6931\n",
      "--- Trial (Exp) 7, Fold 1/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 7, Fold 2/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 7, Fold 3/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 7, Fold 4/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 7, Fold 5/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 22:46:47,251] Trial 7 finished with value: 0.6908977362960222 and parameters: {'learning_rate': 0.00010245675148976868, 'weight_decay': 1.1104219053755356e-06, 'batch_size': 12, 'model': 'AlexNet'}. Best is trial 2 with value: 0.6739577318623196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Average Validation Loss across 5 folds: 0.6909\n",
      "--- Trial (Exp) 8, Fold 1/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 8, Fold 2/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 8, Fold 3/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 8, Fold 4/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 8, Fold 5/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 23:55:57,845] Trial 8 finished with value: 0.6931471824645996 and parameters: {'learning_rate': 0.0005654773571164447, 'weight_decay': 1.920838143302084e-06, 'batch_size': 12, 'model': 'AlexNet'}. Best is trial 2 with value: 0.6739577318623196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Average Validation Loss across 5 folds: 0.6931\n",
      "--- Trial (Exp) 9, Fold 1/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 9, Fold 2/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 9, Fold 3/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 9, Fold 4/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 9, Fold 5/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-19 01:04:21,305] Trial 9 finished with value: 0.6905529202182012 and parameters: {'learning_rate': 0.00011157050475400868, 'weight_decay': 9.386871593134884e-05, 'batch_size': 16, 'model': 'AlexNet'}. Best is trial 2 with value: 0.6739577318623196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Average Validation Loss across 5 folds: 0.6906\n",
      "--- Trial (Exp) 10, Fold 1/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 10, Fold 2/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 10, Fold 3/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 10, Fold 4/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 10, Fold 5/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-19 02:15:02,845] Trial 10 finished with value: 0.6733226219446673 and parameters: {'learning_rate': 3.119404251142601e-05, 'weight_decay': 3.973381283264181e-05, 'batch_size': 8, 'model': 'AlexNet'}. Best is trial 10 with value: 0.6733226219446673.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10, Average Validation Loss across 5 folds: 0.6733\n",
      "New best parameters for AlexNet saved: {'learning_rate': 3.119404251142601e-05, 'weight_decay': 3.973381283264181e-05, 'batch_size': 8, 'model': 'AlexNet'}\n",
      "--- Trial (Exp) 11, Fold 1/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 11, Fold 2/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 11, Fold 3/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 11, Fold 4/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 11, Fold 5/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-19 03:25:53,360] Trial 11 finished with value: 0.6750127537082383 and parameters: {'learning_rate': 3.5341218459578816e-05, 'weight_decay': 3.8657399877027845e-05, 'batch_size': 8, 'model': 'AlexNet'}. Best is trial 10 with value: 0.6733226219446673.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11, Average Validation Loss across 5 folds: 0.6750\n",
      "--- Trial (Exp) 12, Fold 1/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 12, Fold 2/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 12, Fold 3/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 12, Fold 4/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 12, Fold 5/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-19 04:36:45,155] Trial 12 finished with value: 0.6750171016044989 and parameters: {'learning_rate': 4.117814532631192e-05, 'weight_decay': 3.743534881932829e-05, 'batch_size': 8, 'model': 'AlexNet'}. Best is trial 10 with value: 0.6733226219446673.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12, Average Validation Loss across 5 folds: 0.6750\n",
      "--- Trial (Exp) 13, Fold 1/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 13, Fold 2/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 13, Fold 3/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 13, Fold 4/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 13, Fold 5/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-19 05:47:26,012] Trial 13 finished with value: 0.6712148422287679 and parameters: {'learning_rate': 1.3146815506531705e-05, 'weight_decay': 3.3494254041695756e-05, 'batch_size': 8, 'model': 'AlexNet'}. Best is trial 13 with value: 0.6712148422287679.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13, Average Validation Loss across 5 folds: 0.6712\n",
      "New best parameters for AlexNet saved: {'learning_rate': 1.3146815506531705e-05, 'weight_decay': 3.3494254041695756e-05, 'batch_size': 8, 'model': 'AlexNet'}\n",
      "--- Trial (Exp) 14, Fold 1/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 14, Fold 2/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 14, Fold 3/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 14, Fold 4/5 ---\n",
      "train_model_with_hyperparams\n",
      "--- Trial (Exp) 14, Fold 5/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-19 07:02:12,085] Trial 14 finished with value: 0.6704581181874261 and parameters: {'learning_rate': 1.0369291948014812e-05, 'weight_decay': 8.292217788444539e-05, 'batch_size': 4, 'model': 'AlexNet'}. Best is trial 14 with value: 0.6704581181874261.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14, Average Validation Loss across 5 folds: 0.6705\n",
      "New best parameters for AlexNet saved: {'learning_rate': 1.0369291948014812e-05, 'weight_decay': 8.292217788444539e-05, 'batch_size': 4, 'model': 'AlexNet'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "study = optuna.create_study(direction=\"minimize\") \n",
    "study.optimize(objective, n_trials=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6665d4-e17f-4fc3-9c78-d4a97a6bfb41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
