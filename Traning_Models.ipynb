{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483ea1b6-be89-4f97-b4f9-11fc75659d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/gabriel/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabi-dadashev\u001b[0m (\u001b[33mgabi-dadashev-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import unidecode\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import VGG19_Weights\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from datetime import datetime\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "######## \n",
    "\n",
    "from DATA_AND_MODELS import VanGoghDataset,VanGoghModel\n",
    "\n",
    "wandb.login(key='ba8ed449ca151ad3f490026aec87d75b7171a16d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de5d13f3-53e3-4c81-926a-607f7e85c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop_check(patience, best_val_loss, best_val_loss_epoch, current_val_loss, current_val_loss_epoch):\n",
    "    early_stop_flag = False  \n",
    "    if current_val_loss < best_val_loss:\n",
    "        best_val_loss = current_val_loss\n",
    "        best_val_loss_epoch = current_val_loss_epoch\n",
    "    else:\n",
    "        if current_val_loss_epoch - best_val_loss_epoch > patience:\n",
    "            early_stop_flag = True  \n",
    "    return best_val_loss, best_val_loss_epoch, early_stop_flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59dc224e-5dfa-44ee-a277-c843f89e08f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs, patience):\n",
    "    print('train_model_with_hyperparams')\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_val_loss_epoch = 0  \n",
    "    early_stop_flag = False\n",
    "    best_model_state = None  \n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    val_f1s = []\n",
    "    val_aucs = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        \n",
    "        ### Training Loop\n",
    "        model.train() \n",
    "        epoch_train_loss = 0.0 \n",
    "        total_train_samples = 0 \n",
    "        correct_train_predictions = 0 \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            outputs = model(inputs).squeeze(1)  \n",
    "            loss = criterion(outputs.view(-1), labels)\n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "\n",
    "            epoch_train_loss += loss.item() * inputs.size(0)\n",
    "            total_train_samples += inputs.size(0)\n",
    "            \n",
    "            preds_train = (outputs > 0.5).float()\n",
    "            correct_train_predictions += (preds_train == labels).sum().item()\n",
    "\n",
    "        epoch_train_loss /= total_train_samples\n",
    "        train_accuracy = correct_train_predictions / total_train_samples\n",
    "\n",
    "        ### Validation Loop\n",
    "        model.eval()  \n",
    "        epoch_val_loss = 0.0 \n",
    "        total_val_samples = 0 \n",
    "        correct_val_predictions = 0\n",
    "\n",
    "        all_val_labels = []\n",
    "        all_val_preds = []\n",
    "        all_val_probs = []\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for inputs, labels in val_loader: \n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs).squeeze(1)  # outputs shape: (batch_size)\n",
    "                loss = criterion(outputs.view(-1), labels)\n",
    "\n",
    "                epoch_val_loss += loss.item() * inputs.size(0)\n",
    "                total_val_samples += inputs.size(0)\n",
    "                \n",
    "                preds = (outputs > 0.5).float()\n",
    "                \n",
    "                correct_val_predictions += (preds == labels).sum().item()\n",
    "\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                all_val_preds.extend(preds.cpu().numpy())\n",
    "                all_val_probs.extend(outputs.cpu().numpy())\n",
    "\n",
    "        epoch_val_loss /= total_val_samples\n",
    "        val_accuracy = correct_val_predictions / total_val_samples\n",
    "\n",
    "        if len(np.unique(all_val_labels)) > 1:\n",
    "            val_f1 = f1_score(all_val_labels, all_val_preds)\n",
    "            val_auc = roc_auc_score(all_val_labels, all_val_probs)\n",
    "        else:\n",
    "            val_f1 = 0\n",
    "            val_auc = 0\n",
    "\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_f1s.append(val_f1)\n",
    "        val_aucs.append(val_auc)\n",
    "\n",
    "        best_val_loss, best_val_loss_epoch, early_stop_flag = early_stop_check(patience, best_val_loss, best_val_loss_epoch, epoch_val_loss, epoch)\n",
    "        if epoch_val_loss == best_val_loss:\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {epoch_train_loss:.4f}, Val Loss = {epoch_val_loss:.4f}, Train Acc = {train_accuracy:.4f}, Val Acc = {val_accuracy:.4f}, Val F1 = {val_f1:.4f}, Val AUC = {val_auc:.4f}\")\n",
    "\n",
    "        if early_stop_flag: \n",
    "            break \n",
    "\n",
    "    if best_model_state is not None: \n",
    "        now = datetime.now()\n",
    "        torch.save(best_model_state, f\"best_model_trial_{now.strftime('%H:%M:%S')}.pt\") \n",
    "\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs_range, train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs_range, val_f1s, label=\"Validation F1 Score\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(\"Validation F1 Score\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs_range, val_aucs, label=\"Validation AUC\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.title(\"Validation AUC\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return best_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c515a1-aca6-44b4-938b-f1e2df88b02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_csv(\"classes.csv\", encoding=\"utf-8\")\n",
    "classes=classes[['filename', 'artist', 'genre', 'description', 'phash', 'width','height', 'genre_count']].copy()\n",
    "classes_only_post_impressionism = classes[classes[\"filename\"].str.contains('Post_Impressionism', case=False, na=False)].copy()\n",
    "classes_only_post_impressionism['is_van_gogh'] = np.where(classes_only_post_impressionism['artist'] == 'vincent van gogh', 1, 0)\n",
    "classes_only_post_impressionism=classes_only_post_impressionism.reset_index(drop=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8507c62c-e0e3-4e67-b0d0-0182f19b4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VanGoghDataset(dataframe=classes_only_post_impressionism)\n",
    "\n",
    "model_VGG19 = VanGoghModel(device,None,'VGG19').model\n",
    "model_Alex = VanGoghModel(device,None,'AlexNet').model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebbb160-08c7-4aca-b9c6-20e563546c20",
   "metadata": {},
   "source": [
    "***Train VGG19***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2e6ceab-980d-446d-b257-bed545b24dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fold 1/5 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gabriel/GIT_DL/wandb/run-20250222_210047-thos2j4t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gabi-dadashev-tel-aviv-university/Train_VGG19/runs/thos2j4t' target=\"_blank\">VGG19_fold_1</a></strong> to <a href='https://wandb.ai/gabi-dadashev-tel-aviv-university/Train_VGG19' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gabi-dadashev-tel-aviv-university/Train_VGG19' target=\"_blank\">https://wandb.ai/gabi-dadashev-tel-aviv-university/Train_VGG19</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gabi-dadashev-tel-aviv-university/Train_VGG19/runs/thos2j4t' target=\"_blank\">https://wandb.ai/gabi-dadashev-tel-aviv-university/Train_VGG19/runs/thos2j4t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model_with_hyperparams\n",
      "Epoch 1: Train Loss = 0.6934, Val Loss = 0.6925, Train Acc = 0.8430, Val Acc = 0.8502, Val F1 = 0.0207, Val AUC = 0.7857\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 40\u001b[0m\n\u001b[1;32m     24\u001b[0m criterion_VGG19 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m     26\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m     27\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain_VGG19\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m     config\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVGG19_fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 40\u001b[0m best_val_loss_VGG19 \u001b[38;5;241m=\u001b[39m train_model_with_hyperparams(\n\u001b[1;32m     41\u001b[0m     model_VGG19, train_loader, val_loader, optimizer_VGG19, criterion_VGG19,\n\u001b[1;32m     42\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs, patience\u001b[38;5;241m=\u001b[39mpatience\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m fold_losses\u001b[38;5;241m.\u001b[39mappend(best_val_loss_VGG19)\n\u001b[1;32m     47\u001b[0m model_VGG19\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m, in \u001b[0;36mtrain_model_with_hyperparams\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, epochs, patience)\u001b[0m\n\u001b[1;32m     21\u001b[0m total_train_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[1;32m     22\u001b[0m correct_train_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     24\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m<string>:32\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/transforms/transforms.py:973\u001b[0m, in \u001b[0;36mRandomResizedCrop.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03m    img (PIL Image or Tensor): Image to be cropped and resized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;124;03m    PIL Image or Tensor: Randomly cropped and resized image.\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    972\u001b[0m i, j, h, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mratio)\n\u001b[0;32m--> 973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mresized_crop(img, i, j, h, w, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation, antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mantialias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/transforms/functional.py:650\u001b[0m, in \u001b[0;36mresized_crop\u001b[0;34m(img, top, left, height, width, size, interpolation, antialias)\u001b[0m\n\u001b[1;32m    648\u001b[0m     _log_api_usage_once(resized_crop)\n\u001b[1;32m    649\u001b[0m img \u001b[38;5;241m=\u001b[39m crop(img, top, left, height, width)\n\u001b[0;32m--> 650\u001b[0m img \u001b[38;5;241m=\u001b[39m resize(img, size, interpolation, antialias\u001b[38;5;241m=\u001b[39mantialias)\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/transforms/functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mresize(\u001b[38;5;28mtuple\u001b[39m(size[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), interpolation)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/PIL/Image.py:2328\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2317\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2318\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[1;32m   2319\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2320\u001b[0m         )\n\u001b[1;32m   2321\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2322\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2323\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2324\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2325\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2326\u001b[0m         )\n\u001b[0;32m-> 2328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim\u001b[38;5;241m.\u001b[39mresize(size, resample, box))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "patience = 7\n",
    "k_folds = 5  \n",
    "epochs=3\n",
    "\n",
    "\n",
    "learning_rate=0.000022012211629918463\n",
    "weight_decay=0.000001549668920821374\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "fold_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "    \n",
    "    print(f\"--- Fold {fold+1}/{k_folds} ---\")\n",
    "\n",
    "    train_dataset = Subset(dataset, train_idx)\n",
    "    val_dataset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer_VGG19 = optim.Adam(model_VGG19.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion_VGG19 = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"Train_VGG19\",\n",
    "        config={\n",
    "            \"model\": \"VGG19\",  \n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"k_folds\": k_folds,\n",
    "            \"epochs\": epochs,      \n",
    "            \"patience\": patience\n",
    "        },\n",
    "        name=f\"VGG19_fold_{fold+1}\"\n",
    "    )\n",
    "\n",
    "    best_val_loss_VGG19 = train_model_with_hyperparams(\n",
    "        model_VGG19, train_loader, val_loader, optimizer_VGG19, criterion_VGG19,\n",
    "        epochs=epochs, patience=patience\n",
    "    )\n",
    "    \n",
    "    fold_losses.append(best_val_loss_VGG19)\n",
    "    \n",
    "    model_VGG19.eval()\n",
    "    val_true = []\n",
    "    val_pred = []\n",
    "    val_probs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_VGG19(inputs).squeeze(1)  \n",
    "            probs = outputs  \n",
    "            preds = (probs > 0.5).float()\n",
    "    \n",
    "            val_true.extend(labels.cpu().numpy())\n",
    "            val_pred.extend(preds.cpu().numpy())\n",
    "            val_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    \n",
    "    acc = accuracy_score(val_true, val_pred)\n",
    "    auc = roc_auc_score(val_true, val_probs)\n",
    "    f1 = f1_score(val_true, val_pred)\n",
    "\n",
    "    wandb.log({\n",
    "        \"fold\": fold+1,\n",
    "        \"best_val_loss_VGG19\": best_val_loss_VGG19,\n",
    "        \"val_accuracy\": acc,\n",
    "        \"val_auc\": auc,\n",
    "        \"val_f1\": f1\n",
    "    })\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23f7406-57da-4655-93e0-d7707b028c2d",
   "metadata": {},
   "source": [
    "***Train AlexNet***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ac356-1f09-4bd7-8dbd-a44afc1be2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 7\n",
    "k_folds = 5  \n",
    "epochs=100\n",
    "\n",
    "batch_size = 8\n",
    "weight_decay = 1.2852718112074654e-05\n",
    "learning_rate=3.020842532706549e-05\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "fold_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "    \n",
    "    print(f\"--- Trial (Exp) {trial.number}, Fold {fold+1}/{k_folds} ---\")\n",
    "\n",
    "    train_dataset = Subset(dataset, train_idx)\n",
    "    val_dataset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    optimizer_AlexNet = optim.Adam(model_Alex.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion_AlexNet = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"Train_AlexNet\",\n",
    "        config={\n",
    "            \"model\": \"AlexNet\",  \n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"k_folds\": k_folds,\n",
    "            \"epochs\": epochs,      \n",
    "            \"patience\": patience\n",
    "        },\n",
    "        name=f\"Train_AlexNet{fold+1}\"\n",
    "    )\n",
    "\n",
    "    best_val_loss_AlexNet = train_model_with_hyperparams(\n",
    "        model_Alex, train_loader, val_loader, optimizer_AlexNet, criterion_AlexNet, epochs=epochs, patience=patience\n",
    "    )\n",
    "\n",
    "    fold_losses.append(best_val_loss)\n",
    "\n",
    " \n",
    "    model_Alex.eval()\n",
    "    val_true = []\n",
    "    val_pred = []\n",
    "    val_probs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_Alex(inputs).squeeze(1)  \n",
    "            probs = outputs  \n",
    "            preds = (probs > 0.5).float()\n",
    "    \n",
    "            val_true.extend(labels.cpu().numpy())\n",
    "            val_pred.extend(preds.cpu().numpy())\n",
    "            val_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(val_true, val_pred)\n",
    "    auc = roc_auc_score(val_true, val_probs)\n",
    "    f1 = f1_score(val_true, val_pred)\n",
    "\n",
    "    wandb.log({\n",
    "        \"fold\": fold+1,\n",
    "        \"best_val_loss_VGG19\": best_val_loss_AlexNet,\n",
    "        \"val_accuracy\": acc,\n",
    "        \"val_auc\": auc,\n",
    "        \"val_f1\": f1\n",
    "    })\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
