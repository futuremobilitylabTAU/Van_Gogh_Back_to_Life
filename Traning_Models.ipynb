{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "483ea1b6-be89-4f97-b4f9-11fc75659d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/gabriel/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import unidecode\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import VGG19_Weights\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "######## \n",
    "\n",
    "from DATA_AND_MODELS import VanGoghDataset,VanGoghModel\n",
    "\n",
    "wandb.login(key='ba8ed449ca151ad3f490026aec87d75b7171a16d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de5d13f3-53e3-4c81-926a-607f7e85c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop_check(patience, best_val_loss, best_val_loss_epoch, current_val_loss, current_val_loss_epoch):\n",
    "    early_stop_flag = False  \n",
    "    if current_val_loss < best_val_loss:\n",
    "        best_val_loss = current_val_loss\n",
    "        best_val_loss_epoch = current_val_loss_epoch\n",
    "    else:\n",
    "        if current_val_loss_epoch - best_val_loss_epoch > patience:\n",
    "            early_stop_flag = True  \n",
    "    return best_val_loss, best_val_loss_epoch, early_stop_flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5c515a1-aca6-44b4-938b-f1e2df88b02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_csv(\"classes.csv\", encoding=\"utf-8\")\n",
    "classes=classes[['filename', 'artist', 'genre', 'description', 'phash', 'width','height', 'genre_count']].copy()\n",
    "classes_only_post_impressionism = classes[classes[\"filename\"].str.contains('Post_Impressionism', case=False, na=False)].copy()\n",
    "classes_only_post_impressionism['is_van_gogh'] = np.where(classes_only_post_impressionism['artist'] == 'vincent van gogh', 1, 0)\n",
    "classes_only_post_impressionism=classes_only_post_impressionism.reset_index(drop=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59dc224e-5dfa-44ee-a277-c843f89e08f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs, patience):\n",
    "\n",
    "    print('train_model_with_hyperparams')\n",
    "    \n",
    "    \n",
    "    best_val_loss = float('inf')  \n",
    "    best_val_loss_epoch = 0  \n",
    "    early_stop_flag = False\n",
    "    best_model_state = None  \n",
    "\n",
    "    ### Epoch Loop iterate the all data observation X time\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train() \n",
    "        train_loss = 0.0 \n",
    "        total_train_samples = 0 \n",
    "        correct_train_predictions = 0 \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "           \n",
    "            \n",
    "            outputs = model(inputs).squeeze(1)  \n",
    "            loss = criterion(outputs.view(-1), labels)\n",
    "\n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            total_train_samples += inputs.size(0)\n",
    "\n",
    "      \n",
    "            predicted = (outputs > 0).float() \n",
    "\n",
    "            correct_train_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= total_train_samples\n",
    "        train_accuracy = correct_train_predictions / total_train_samples\n",
    "\n",
    "        model.eval()  \n",
    "        val_loss = 0.0 \n",
    "        total_val_samples = 0 \n",
    "        correct_val_predictions = 0\n",
    "\n",
    "        ## Intenial-Batch Loop split the data to batches\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for inputs, labels in val_loader: \n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "               \n",
    "            \n",
    "               \n",
    "                loss = criterion(outputs.view(-1), labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                total_val_samples += inputs.size(0)\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        \n",
    "        ##Validation \n",
    "        \n",
    "        val_loss /= total_val_samples\n",
    "        val_accuracy = correct_val_predictions / total_val_samples\n",
    "\n",
    "        best_val_loss, best_val_loss_epoch, early_stop_flag = early_stop_check(patience, best_val_loss, best_val_loss_epoch, val_loss, epoch)\n",
    "\n",
    "        ##Early Stopping\n",
    "        \n",
    "        if val_loss == best_val_loss:\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        \n",
    "\n",
    "        if early_stop_flag: \n",
    "            break \n",
    "\n",
    "        ### handle best model\n",
    "        \n",
    "\n",
    "    if best_model_state is not None: \n",
    "        torch.save(best_model_state, f\"best_model_trial_{trial.number}.pt\") \n",
    "   \n",
    "\n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8507c62c-e0e3-4e67-b0d0-0182f19b4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VanGoghDataset(dataframe=classes_only_post_impressionism)\n",
    "\n",
    "model_VGG19 = VanGoghModel(device,None,'VGG19').model\n",
    "model_Alex = VanGoghModel(device,None,'AlexNet').model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebbb160-08c7-4aca-b9c6-20e563546c20",
   "metadata": {},
   "source": [
    "***Train VGG19***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6ceab-980d-446d-b257-bed545b24dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fold 1/5 ---\n",
      "train_model_with_hyperparams\n"
     ]
    }
   ],
   "source": [
    "patience = 7\n",
    "k_folds = 5  \n",
    "epochs=100\n",
    "\n",
    "\n",
    "learning_rate=0.000022012211629918463\n",
    "weight_decay=0.000001549668920821374\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "fold_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "    \n",
    "    print(f\"--- Fold {fold+1}/{k_folds} ---\")\n",
    "\n",
    "    train_dataset = Subset(dataset, train_idx)\n",
    "    val_dataset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer_VGG19 = optim.Adam(model_VGG19.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion_VGG19 = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"Train_VGG19\",\n",
    "        config={\n",
    "            \"model\": \"VGG19\",  \n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"k_folds\": k_folds,\n",
    "            \"epochs\": epochs,      \n",
    "            \"patience\": patience\n",
    "        },\n",
    "        name=f\"VGG19_fold_{fold+1}\"\n",
    "    )\n",
    "\n",
    "    best_val_loss_VGG19 = train_model_with_hyperparams(\n",
    "        model_VGG19, train_loader, val_loader, optimizer_VGG19, criterion_VGG19,\n",
    "        epochs=epochs, patience=patience\n",
    "    )\n",
    "    \n",
    "    fold_losses.append(best_val_loss_VGG19)\n",
    "    \n",
    "    model_VGG19.eval()\n",
    "    val_true = []\n",
    "    val_pred = []\n",
    "    val_probs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_VGG19(inputs).squeeze(1)\n",
    "            probs = torch.sigmoid(outputs)  \n",
    "            preds = (probs > 0.5).float()     \n",
    "\n",
    "            val_true.extend(labels.cpu().numpy())\n",
    "            val_pred.extend(preds.cpu().numpy())\n",
    "            val_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(val_true, val_pred)\n",
    "    auc = roc_auc_score(val_true, val_probs)\n",
    "    f1 = f1_score(val_true, val_pred)\n",
    "\n",
    "    wandb.log({\n",
    "        \"fold\": fold+1,\n",
    "        \"best_val_loss_VGG19\": best_val_loss_VGG19,\n",
    "        \"val_accuracy\": acc,\n",
    "        \"val_auc\": auc,\n",
    "        \"val_f1\": f1\n",
    "    })\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23f7406-57da-4655-93e0-d7707b028c2d",
   "metadata": {},
   "source": [
    "***Train AlexNet***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ac356-1f09-4bd7-8dbd-a44afc1be2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 7\n",
    "k_folds = 5  \n",
    "epochs=100\n",
    "\n",
    "batch_size = 8\n",
    "weight_decay = 1.2852718112074654e-05\n",
    "learning_rate=3.020842532706549e-05\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "fold_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "    \n",
    "    print(f\"--- Trial (Exp) {trial.number}, Fold {fold+1}/{k_folds} ---\")\n",
    "\n",
    "    train_dataset = Subset(dataset, train_idx)\n",
    "    val_dataset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    optimizer_AlexNet = optim.Adam(model_Alex.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion_AlexNet = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"Train_AlexNet\",\n",
    "        config={\n",
    "            \"model\": trial.params[\"model\"],\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"k_folds\": k_folds,\n",
    "            \"epochs\": epochs,      \n",
    "            \"patience\": patience\n",
    "        },\n",
    "        name=f\"trial_{trial.number}_{trial.params['model']}_fold_{fold+1}\"\n",
    "    )\n",
    "\n",
    "    best_val_loss_AlexNet = train_model_with_hyperparams(\n",
    "        model_Alex, train_loader, val_loader, optimizer_AlexNet, criterion_AlexNet, epochs=epochs, patience=patience, trial=trial\n",
    "    )\n",
    "\n",
    "    fold_losses.append(best_val_loss)\n",
    "\n",
    " \n",
    "    model_Alex.eval()\n",
    "    val_true = []\n",
    "    val_pred = []\n",
    "    val_probs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_VGG19(inputs).squeeze(1)\n",
    "            probs = torch.sigmoid(outputs)  \n",
    "            preds = (probs > 0.5).float()     \n",
    "\n",
    "            val_true.extend(labels.cpu().numpy())\n",
    "            val_pred.extend(preds.cpu().numpy())\n",
    "            val_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(val_true, val_pred)\n",
    "    auc = roc_auc_score(val_true, val_probs)\n",
    "    f1 = f1_score(val_true, val_pred)\n",
    "\n",
    "    wandb.log({\n",
    "        \"fold\": fold+1,\n",
    "        \"best_val_loss_VGG19\": best_val_loss_AlexNet,\n",
    "        \"val_accuracy\": acc,\n",
    "        \"val_auc\": auc,\n",
    "        \"val_f1\": f1\n",
    "    })\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
