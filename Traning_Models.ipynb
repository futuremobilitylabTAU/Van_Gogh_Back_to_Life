{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483ea1b6-be89-4f97-b4f9-11fc75659d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/gabriel/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabi-dadashev\u001b[0m (\u001b[33mgabi-dadashev-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import unidecode\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import VGG19_Weights\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from datetime import datetime\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "######## \n",
    "\n",
    "from DATA_AND_MODELS import VanGoghDataset,VanGoghModel\n",
    "\n",
    "wandb.login(key='ba8ed449ca151ad3f490026aec87d75b7171a16d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de5d13f3-53e3-4c81-926a-607f7e85c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop_check(patience, best_val_loss, best_val_loss_epoch, current_val_loss, current_val_loss_epoch):\n",
    "    early_stop_flag = False  \n",
    "    if current_val_loss < best_val_loss:\n",
    "        best_val_loss = current_val_loss\n",
    "        best_val_loss_epoch = current_val_loss_epoch\n",
    "    else:\n",
    "        if current_val_loss_epoch - best_val_loss_epoch > patience:\n",
    "            early_stop_flag = True  \n",
    "    return best_val_loss, best_val_loss_epoch, early_stop_flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59dc224e-5dfa-44ee-a277-c843f89e08f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs, patience):\n",
    "    print('train_model_with_hyperparams')\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_val_loss_epoch = 0  \n",
    "    early_stop_flag = False\n",
    "    best_model_state = None  \n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    val_f1s = []\n",
    "    val_aucs = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        \n",
    "        ### Training Loop\n",
    "        model.train() \n",
    "        epoch_train_loss = 0.0 \n",
    "        total_train_samples = 0 \n",
    "        correct_train_predictions = 0 \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            outputs = model(inputs).squeeze(1)  # outputs shape: (batch_size)\n",
    "            loss = criterion(outputs.view(-1), labels)\n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "\n",
    "            epoch_train_loss += loss.item() * inputs.size(0)\n",
    "            total_train_samples += inputs.size(0)\n",
    "            \n",
    "            # מאחר והמודל כבר מפעיל Sigmoid, אין צורך להפעיל אותו שוב\n",
    "            preds_train = (outputs > 0.5).float()\n",
    "            correct_train_predictions += (preds_train == labels).sum().item()\n",
    "\n",
    "        epoch_train_loss /= total_train_samples\n",
    "        train_accuracy = correct_train_predictions / total_train_samples\n",
    "\n",
    "        ### Validation Loop\n",
    "        model.eval()  \n",
    "        epoch_val_loss = 0.0 \n",
    "        total_val_samples = 0 \n",
    "        correct_val_predictions = 0\n",
    "\n",
    "        all_val_labels = []\n",
    "        all_val_preds = []\n",
    "        all_val_probs = []\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for inputs, labels in val_loader: \n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs).squeeze(1)  # outputs shape: (batch_size)\n",
    "                loss = criterion(outputs.view(-1), labels)\n",
    "\n",
    "                epoch_val_loss += loss.item() * inputs.size(0)\n",
    "                total_val_samples += inputs.size(0)\n",
    "                \n",
    "                # מאחר והמודל כבר מחזיר הסתברויות, נשתמש ב־outputs ישירות\n",
    "                preds = (outputs > 0.5).float()\n",
    "                \n",
    "                correct_val_predictions += (preds == labels).sum().item()\n",
    "\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                all_val_preds.extend(preds.cpu().numpy())\n",
    "                all_val_probs.extend(outputs.cpu().numpy())\n",
    "\n",
    "        epoch_val_loss /= total_val_samples\n",
    "        val_accuracy = correct_val_predictions / total_val_samples\n",
    "\n",
    "        if len(np.unique(all_val_labels)) > 1:\n",
    "            val_f1 = f1_score(all_val_labels, all_val_preds)\n",
    "            val_auc = roc_auc_score(all_val_labels, all_val_probs)\n",
    "        else:\n",
    "            val_f1 = 0\n",
    "            val_auc = 0\n",
    "\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_f1s.append(val_f1)\n",
    "        val_aucs.append(val_auc)\n",
    "\n",
    "        best_val_loss, best_val_loss_epoch, early_stop_flag = early_stop_check(patience, best_val_loss, best_val_loss_epoch, epoch_val_loss, epoch)\n",
    "        if epoch_val_loss == best_val_loss:\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {epoch_train_loss:.4f}, Val Loss = {epoch_val_loss:.4f}, Train Acc = {train_accuracy:.4f}, Val Acc = {val_accuracy:.4f}, Val F1 = {val_f1:.4f}, Val AUC = {val_auc:.4f}\")\n",
    "\n",
    "        if early_stop_flag: \n",
    "            break \n",
    "\n",
    "    if best_model_state is not None: \n",
    "        now = datetime.now()\n",
    "        torch.save(best_model_state, f\"best_model_trial_{now.strftime('%H:%M:%S')}.pt\") \n",
    "\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs_range, train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs_range, val_f1s, label=\"Validation F1 Score\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(\"Validation F1 Score\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs_range, val_aucs, label=\"Validation AUC\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.title(\"Validation AUC\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return best_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c515a1-aca6-44b4-938b-f1e2df88b02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_csv(\"classes.csv\", encoding=\"utf-8\")\n",
    "classes=classes[['filename', 'artist', 'genre', 'description', 'phash', 'width','height', 'genre_count']].copy()\n",
    "classes_only_post_impressionism = classes[classes[\"filename\"].str.contains('Post_Impressionism', case=False, na=False)].copy()\n",
    "classes_only_post_impressionism['is_van_gogh'] = np.where(classes_only_post_impressionism['artist'] == 'vincent van gogh', 1, 0)\n",
    "classes_only_post_impressionism=classes_only_post_impressionism.reset_index(drop=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8507c62c-e0e3-4e67-b0d0-0182f19b4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VanGoghDataset(dataframe=classes_only_post_impressionism)\n",
    "\n",
    "model_VGG19 = VanGoghModel(device,None,'VGG19').model\n",
    "model_Alex = VanGoghModel(device,None,'AlexNet').model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebbb160-08c7-4aca-b9c6-20e563546c20",
   "metadata": {},
   "source": [
    "***Train VGG19***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6ceab-980d-446d-b257-bed545b24dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fold 1/5 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gabriel/GIT_DL/wandb/run-20250222_205737-bzl5tm1d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gabi-dadashev-tel-aviv-university/Train_VGG19/runs/bzl5tm1d' target=\"_blank\">VGG19_fold_1</a></strong> to <a href='https://wandb.ai/gabi-dadashev-tel-aviv-university/Train_VGG19' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gabi-dadashev-tel-aviv-university/Train_VGG19' target=\"_blank\">https://wandb.ai/gabi-dadashev-tel-aviv-university/Train_VGG19</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gabi-dadashev-tel-aviv-university/Train_VGG19/runs/bzl5tm1d' target=\"_blank\">https://wandb.ai/gabi-dadashev-tel-aviv-university/Train_VGG19/runs/bzl5tm1d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model_with_hyperparams\n"
     ]
    }
   ],
   "source": [
    "patience = 7\n",
    "k_folds = 5  \n",
    "epochs=3\n",
    "\n",
    "\n",
    "learning_rate=0.000022012211629918463\n",
    "weight_decay=0.000001549668920821374\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "fold_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "    \n",
    "    print(f\"--- Fold {fold+1}/{k_folds} ---\")\n",
    "\n",
    "    train_dataset = Subset(dataset, train_idx)\n",
    "    val_dataset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer_VGG19 = optim.Adam(model_VGG19.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion_VGG19 = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"Train_VGG19\",\n",
    "        config={\n",
    "            \"model\": \"VGG19\",  \n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"k_folds\": k_folds,\n",
    "            \"epochs\": epochs,      \n",
    "            \"patience\": patience\n",
    "        },\n",
    "        name=f\"VGG19_fold_{fold+1}\"\n",
    "    )\n",
    "\n",
    "    best_val_loss_VGG19 = train_model_with_hyperparams(\n",
    "        model_VGG19, train_loader, val_loader, optimizer_VGG19, criterion_VGG19,\n",
    "        epochs=epochs, patience=patience\n",
    "    )\n",
    "    \n",
    "    fold_losses.append(best_val_loss_VGG19)\n",
    "    \n",
    "    model_VGG19.eval()\n",
    "    val_true = []\n",
    "    val_pred = []\n",
    "    val_probs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_VGG19(inputs).squeeze(1)\n",
    "            probs = torch.sigmoid(outputs)  \n",
    "            preds = (probs > 0.5).float()     \n",
    "\n",
    "            val_true.extend(labels.cpu().numpy())\n",
    "            val_pred.extend(preds.cpu().numpy())\n",
    "            val_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(val_true, val_pred)\n",
    "    auc = roc_auc_score(val_true, val_probs)\n",
    "    f1 = f1_score(val_true, val_pred)\n",
    "\n",
    "    wandb.log({\n",
    "        \"fold\": fold+1,\n",
    "        \"best_val_loss_VGG19\": best_val_loss_VGG19,\n",
    "        \"val_accuracy\": acc,\n",
    "        \"val_auc\": auc,\n",
    "        \"val_f1\": f1\n",
    "    })\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23f7406-57da-4655-93e0-d7707b028c2d",
   "metadata": {},
   "source": [
    "***Train AlexNet***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ac356-1f09-4bd7-8dbd-a44afc1be2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 7\n",
    "k_folds = 5  \n",
    "epochs=100\n",
    "\n",
    "batch_size = 8\n",
    "weight_decay = 1.2852718112074654e-05\n",
    "learning_rate=3.020842532706549e-05\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "fold_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "    \n",
    "    print(f\"--- Trial (Exp) {trial.number}, Fold {fold+1}/{k_folds} ---\")\n",
    "\n",
    "    train_dataset = Subset(dataset, train_idx)\n",
    "    val_dataset = Subset(dataset, val_idx)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    optimizer_AlexNet = optim.Adam(model_Alex.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion_AlexNet = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"Train_AlexNet\",\n",
    "        config={\n",
    "            \"model\": \"AlexNet\",  \n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"k_folds\": k_folds,\n",
    "            \"epochs\": epochs,      \n",
    "            \"patience\": patience\n",
    "        },\n",
    "        name=f\"Train_AlexNet{fold+1}\"\n",
    "    )\n",
    "\n",
    "    best_val_loss_AlexNet = train_model_with_hyperparams(\n",
    "        model_Alex, train_loader, val_loader, optimizer_AlexNet, criterion_AlexNet, epochs=epochs, patience=patience\n",
    "    )\n",
    "\n",
    "    fold_losses.append(best_val_loss)\n",
    "\n",
    " \n",
    "    model_Alex.eval()\n",
    "    val_true = []\n",
    "    val_pred = []\n",
    "    val_probs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_VGG19(inputs).squeeze(1)\n",
    "            probs = torch.sigmoid(outputs)  \n",
    "            preds = (probs > 0.5).float()     \n",
    "\n",
    "            val_true.extend(labels.cpu().numpy())\n",
    "            val_pred.extend(preds.cpu().numpy())\n",
    "            val_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(val_true, val_pred)\n",
    "    auc = roc_auc_score(val_true, val_probs)\n",
    "    f1 = f1_score(val_true, val_pred)\n",
    "\n",
    "    wandb.log({\n",
    "        \"fold\": fold+1,\n",
    "        \"best_val_loss_VGG19\": best_val_loss_AlexNet,\n",
    "        \"val_accuracy\": acc,\n",
    "        \"val_auc\": auc,\n",
    "        \"val_f1\": f1\n",
    "    })\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
